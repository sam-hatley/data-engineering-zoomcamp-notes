{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/03 10:30:11 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, types\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_green = spark.read.parquet('./data/pq/green/*/*/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_green.registerTempTable('green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DATE_TRUNC('hour', lpep_pickup_datetime) AS hour, \n",
    "        PULocationID AS revenue_zone,\n",
    "        ROUND(SUM(total_amount), 2) AS amount,\n",
    "        COUNT(*) AS number_records\n",
    "    FROM green\n",
    "    WHERE lpep_pickup_datetime >= '2020-01-01'\n",
    "    GROUP BY\n",
    "        hour,\n",
    "        revenue_zone\n",
    "    ORDER BY\n",
    "        hour,\n",
    "        revenue_zone\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-------+--------------+\n",
      "|               hour|revenue_zone| amount|number_records|\n",
      "+-------------------+------------+-------+--------------+\n",
      "|2020-01-01 00:00:00|           7| 769.73|            45|\n",
      "|2020-01-01 00:00:00|          17| 195.03|             9|\n",
      "|2020-01-01 00:00:00|          18|    7.8|             1|\n",
      "|2020-01-01 00:00:00|          22|   15.8|             1|\n",
      "|2020-01-01 00:00:00|          24|   87.6|             3|\n",
      "|2020-01-01 00:00:00|          25|  531.0|            26|\n",
      "|2020-01-01 00:00:00|          29|   61.3|             1|\n",
      "|2020-01-01 00:00:00|          32|  68.95|             2|\n",
      "|2020-01-01 00:00:00|          33| 317.27|            11|\n",
      "|2020-01-01 00:00:00|          35| 129.96|             5|\n",
      "|2020-01-01 00:00:00|          36| 295.34|            11|\n",
      "|2020-01-01 00:00:00|          37| 175.67|             6|\n",
      "|2020-01-01 00:00:00|          38|  98.79|             2|\n",
      "|2020-01-01 00:00:00|          40| 168.98|             8|\n",
      "|2020-01-01 00:00:00|          41|1363.96|            84|\n",
      "|2020-01-01 00:00:00|          42| 799.76|            52|\n",
      "|2020-01-01 00:00:00|          43| 107.52|             6|\n",
      "|2020-01-01 00:00:00|          47|   13.3|             1|\n",
      "|2020-01-01 00:00:00|          49| 266.76|            14|\n",
      "|2020-01-01 00:00:00|          51|   17.8|             2|\n",
      "+-------------------+------------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.write.parquet('./data/report/revenue/green/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's happening above:\n",
    "    - Partitions are sent to executors\n",
    "    - executors first apply the filters we specify, then run transformations to provide intermediate results\n",
    "    - intermediate results are *reshuffled* to executors, in this case based on hour and zone: these must end up in the same output partition \n",
    "    - if there's an `ORDER BY` command, another stage will order "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_yellow = spark.read.parquet('./data/pq/yellow/*/*/')\n",
    "df_yellow.registerTempTable('yellow')\n",
    "\n",
    "df_result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DATE_TRUNC('hour', tpep_pickup_datetime) AS hour, \n",
    "        PULocationID AS revenue_zone,\n",
    "        ROUND(SUM(total_amount), 2) AS amount,\n",
    "        COUNT(*) AS number_records\n",
    "    FROM yellow\n",
    "    WHERE tpep_pickup_datetime >= '2020-01-01'\n",
    "    GROUP BY\n",
    "        hour,\n",
    "        revenue_zone\n",
    "    ORDER BY\n",
    "        hour,\n",
    "        revenue_zone\n",
    "\"\"\")\n",
    "\n",
    "df_result.write.parquet('./data/report/revenue/yellow/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:===================================================>    (11 + 1) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------------+-------+--------------+\n",
      "|               hour|revenue_zone| amount|number_records|\n",
      "+-------------------+------------+-------+--------------+\n",
      "|2020-01-01 00:00:00|           3|   25.0|             1|\n",
      "|2020-01-01 00:00:00|           4| 1004.3|            57|\n",
      "|2020-01-01 00:00:00|           7| 455.17|            38|\n",
      "|2020-01-01 00:00:00|          10|  42.41|             2|\n",
      "|2020-01-01 00:00:00|          12|  107.0|             6|\n",
      "|2020-01-01 00:00:00|          13| 1214.8|            56|\n",
      "|2020-01-01 00:00:00|          14|    8.8|             1|\n",
      "|2020-01-01 00:00:00|          15|  34.09|             1|\n",
      "|2020-01-01 00:00:00|          17| 220.21|             8|\n",
      "|2020-01-01 00:00:00|          18|    5.8|             1|\n",
      "|2020-01-01 00:00:00|          24| 754.95|            45|\n",
      "|2020-01-01 00:00:00|          25| 324.35|            16|\n",
      "|2020-01-01 00:00:00|          32|   18.0|             1|\n",
      "|2020-01-01 00:00:00|          33| 255.56|             8|\n",
      "|2020-01-01 00:00:00|          34|   19.3|             1|\n",
      "|2020-01-01 00:00:00|          36| 109.17|             3|\n",
      "|2020-01-01 00:00:00|          37| 161.61|             7|\n",
      "|2020-01-01 00:00:00|          40|  89.97|             5|\n",
      "|2020-01-01 00:00:00|          41|1256.53|            80|\n",
      "|2020-01-01 00:00:00|          42| 635.35|            46|\n",
      "+-------------------+------------+-------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_result.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Homework\n",
    "\n",
    ">For this homework we will be using the FHVHV 2021-06 data found here. [FHVHV Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-03-05 07:56:32--  https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz\n",
      "Resolving github.com (github.com)... 140.82.121.3\n",
      "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/4564ad9e-a6da-4923-ad6f-35ff02446a51?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230305%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230305T075632Z&X-Amz-Expires=300&X-Amz-Signature=820688a9a8d437d4b3048401f04983835bc2c56766f6be81a632680fc42f604c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-06.csv.gz&response-content-type=application%2Foctet-stream [following]\n",
      "--2023-03-05 07:56:32--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/513814948/4564ad9e-a6da-4923-ad6f-35ff02446a51?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20230305%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20230305T075632Z&X-Amz-Expires=300&X-Amz-Signature=820688a9a8d437d4b3048401f04983835bc2c56766f6be81a632680fc42f604c&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=513814948&response-content-disposition=attachment%3B%20filename%3Dfhvhv_tripdata_2021-06.csv.gz&response-content-type=application%2Foctet-stream\n",
      "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 175799316 (168M) [application/octet-stream]\n",
      "Saving to: ‘./data/fhvhv_2021-06.csv.gz’\n",
      "\n",
      "./data/fhvhv_2021-0 100%[===================>] 167.66M  35.0MB/s    in 5.4s    \n",
      "\n",
      "2023-03-05 07:56:38 (30.8 MB/s) - ‘./data/fhvhv_2021-06.csv.gz’ saved [175799316/175799316]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhvhv/fhvhv_tripdata_2021-06.csv.gz -O ./data/fhvhv_2021-06.csv.gz"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: \n",
    "\n",
    ">**Install Spark and PySpark** \n",
    ">- Install Spark\n",
    ">- Run PySpark\n",
    ">- Create a local spark session\n",
    ">- Execute spark.version.\n",
    ">\n",
    ">What's the output?\n",
    "- 3.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/03/05 07:48:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession, types\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()\n",
    "    \n",
    "spark.version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: \n",
    "\n",
    ">**HVFHW June 2021**\n",
    ">\n",
    ">Read it with Spark using the same schema as we did in the lessons.</br> \n",
    ">We will use this dataset for all the remaining questions.</br>\n",
    ">Repartition it to 12 partitions and save it to parquet.</br>\n",
    ">What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.</br>\n",
    "\n",
    "- 24MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dispatching_base_num      object\n",
       "pickup_datetime           object\n",
       "dropoff_datetime          object\n",
       "PULocationID               int64\n",
       "DOLocationID               int64\n",
       "SR_Flag                   object\n",
       "Affiliated_base_number    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_pd = pd.read_csv('./data/fhvhv_2021-06.csv.gz', \\\n",
    "    nrows = 100)\n",
    "\n",
    "schema = df_pd.dtypes\n",
    "schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 6, 1, 0, 2, 41), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 7, 46), PULocationID=174, DOLocationID=18, SR_Flag='N', Affiliated_base_number='B02764'),\n",
       " Row(dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 6, 1, 0, 16, 16), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 21, 14), PULocationID=32, DOLocationID=254, SR_Flag='N', Affiliated_base_number='B02764'),\n",
       " Row(dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 6, 1, 0, 27, 1), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 42, 11), PULocationID=240, DOLocationID=127, SR_Flag='N', Affiliated_base_number='B02764'),\n",
       " Row(dispatching_base_num='B02764', pickup_datetime=datetime.datetime(2021, 6, 1, 0, 46, 8), dropoff_datetime=datetime.datetime(2021, 6, 1, 0, 53, 45), PULocationID=127, DOLocationID=235, SR_Flag='N', Affiliated_base_number='B02764'),\n",
       " Row(dispatching_base_num='B02510', pickup_datetime=datetime.datetime(2021, 6, 1, 0, 45, 42), dropoff_datetime=datetime.datetime(2021, 6, 1, 1, 3, 33), PULocationID=144, DOLocationID=146, SR_Flag='N', Affiliated_base_number=None)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType([\n",
    "types.StructField('dispatching_base_num', types.StringType(), True), \n",
    "types.StructField('pickup_datetime', types.TimestampType(), True), \n",
    "types.StructField('dropoff_datetime', types.TimestampType(), True), \n",
    "types.StructField('PULocationID', types.LongType(), True), \n",
    "types.StructField('DOLocationID', types.LongType(), True), \n",
    "types.StructField('SR_Flag', types.StringType(), True),\n",
    "types.StructField('Affiliated_base_number', types.StringType(), True), \n",
    "])\n",
    "\n",
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(\"./data/fhvhv_2021-06.csv.gz\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 284M\n",
      "-rw-r--r-- 1 sam sam   0 Mar  5 08:03 _SUCCESS\n",
      "-rw-r--r-- 1 sam sam 24M Mar  5 08:03 part-00000-a58b5092-2344-4cab-9f0a-64a8129e337a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 sam sam 24M Mar  5 08:03 part-00001-a58b5092-2344-4cab-9f0a-64a8129e337a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 sam sam 24M Mar  5 08:03 part-00002-a58b5092-2344-4cab-9f0a-64a8129e337a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 sam sam 24M Mar  5 08:03 part-00003-a58b5092-2344-4cab-9f0a-64a8129e337a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 sam sam 24M Mar  5 08:03 part-00004-a58b5092-2344-4cab-9f0a-64a8129e337a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 sam sam 24M Mar  5 08:03 part-00005-a58b5092-2344-4cab-9f0a-64a8129e337a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 sam sam 24M Mar  5 08:03 part-00006-a58b5092-2344-4cab-9f0a-64a8129e337a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 sam sam 24M Mar  5 08:03 part-00007-a58b5092-2344-4cab-9f0a-64a8129e337a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 sam sam 24M Mar  5 08:03 part-00008-a58b5092-2344-4cab-9f0a-64a8129e337a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 sam sam 24M Mar  5 08:03 part-00009-a58b5092-2344-4cab-9f0a-64a8129e337a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 sam sam 24M Mar  5 08:03 part-00010-a58b5092-2344-4cab-9f0a-64a8129e337a-c000.snappy.parquet\n",
      "-rw-r--r-- 1 sam sam 24M Mar  5 08:03 part-00011-a58b5092-2344-4cab-9f0a-64a8129e337a-c000.snappy.parquet\n"
     ]
    }
   ],
   "source": [
    "df = df.repartition(12)\n",
    "df.write.parquet('./data/fhv_pq')\n",
    "!ls -lh ./data/fhv_pq"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: \n",
    "\n",
    ">**Count records**  \n",
    ">\n",
    ">How many taxi trips were there on June 15?</br></br>\n",
    ">Consider only trips that started on June 15.</br>\n",
    "\n",
    "- 452,470"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df.registerTempTable('fhvhv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| trips|\n",
      "+------+\n",
      "|452470|\n",
      "+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) AS trips\n",
    "    FROM fhvhv\n",
    "    WHERE pickup_datetime >= '2021-06-15'\n",
    "        AND pickup_datetime < '2021-06-16'\n",
    "\"\"\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: \n",
    "\n",
    ">**Longest trip for each day**  \n",
    ">\n",
    ">Now calculate the duration for each trip.</br>\n",
    ">How long was the longest trip in Hours?</br>\n",
    "\n",
    "- 66.87 Hours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|           TimeDiff|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-------------------+\n",
      "|              B02879|2021-06-26 10:55:58|2021-06-26 11:07:36|          48|         162|      N|                B02879| 0.1938888888888889|\n",
      "|              B02864|2021-06-04 14:18:02|2021-06-04 15:04:26|         231|         138|      N|                B02864| 0.7733333333333333|\n",
      "|              B02510|2021-06-11 00:43:47|2021-06-11 00:54:15|           4|         144|      N|                  null|0.17444444444444446|\n",
      "|              B02875|2021-06-10 14:13:55|2021-06-10 15:18:21|         164|          17|      N|                B02875|  1.073888888888889|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+-------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Trying this with a function, rather than straight SQL\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "df_dur = df.withColumn('TimeDiff', \\\n",
    "    (F.col('dropoff_datetime').cast('long') - \\\n",
    "    F.col('pickup_datetime').cast('long'))/3600)\n",
    "df_dur.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n",
      "[Stage 25:======================================>                  (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+--------+\n",
      "|dispatching_base_num|    pickup_datetime|TimeDiff|\n",
      "+--------------------+-------------------+--------+\n",
      "|              B02872|2021-06-25 13:55:41|   66.88|\n",
      "|              B02765|2021-06-22 12:09:45|   25.55|\n",
      "|              B02879|2021-06-27 10:32:29|   19.98|\n",
      "|              B02800|2021-06-26 22:37:11|    18.2|\n",
      "|              B02682|2021-06-23 20:40:43|   16.47|\n",
      "+--------------------+-------------------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_dur.registerTempTable('fhvhv_dur')\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        dispatching_base_num,\n",
    "        pickup_datetime,\n",
    "        ROUND(TimeDiff, 2) AS TimeDiff\n",
    "    FROM fhvhv_dur\n",
    "    ORDER BY TimeDiff DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: \n",
    "\n",
    ">**User Interface**\n",
    ">\n",
    ">Spark’s User Interface which shows application's dashboard runs on which local port?</br>\n",
    "\n",
    "- 4040"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6: \n",
    "\n",
    ">**Most frequent pickup location zone**\n",
    ">\n",
    ">Load the zone lookup data into a temp view in Spark</br>\n",
    ">[Zone Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv)</br>\n",
    ">\n",
    ">Using the zone lookup data and the fhvhv June 2021 data, what is the name of the most frequent pickup location zone?</br>\n",
    "\n",
    "- Crown Heights North"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sam/anaconda3/envs/pyspark/lib/python3.9/site-packages/pyspark/sql/dataframe.py:229: FutureWarning: Deprecated in 2.0, use createOrReplaceTempView instead.\n",
      "  warnings.warn(\"Deprecated in 2.0, use createOrReplaceTempView instead.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "df_zones = spark.read.csv('./data/raw/taxi_zone_lookup.csv', header=True)\n",
    "df.join(df_zones, df.PULocationID == df_zones.LocationID, how='inner') \\\n",
    "    .registerTempTable('df_join')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:==============================================>         (10 + 2) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "| trips|                Zone|\n",
      "+------+--------------------+\n",
      "|231279| Crown Heights North|\n",
      "|221244|        East Village|\n",
      "|188867|         JFK Airport|\n",
      "|187929|      Bushwick South|\n",
      "|186780|       East New York|\n",
      "|164344|TriBeCa/Civic Center|\n",
      "|161596|   LaGuardia Airport|\n",
      "|158937|            Union Sq|\n",
      "|154698|        West Village|\n",
      "|152493|             Astoria|\n",
      "+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        COUNT(*) AS trips,\n",
    "        Zone\n",
    "    FROM df_join\n",
    "    GROUP BY Zone\n",
    "    ORDER BY trips DESC\n",
    "\"\"\").show(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
